{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap: What we've been doing\n",
    "\n",
    "\n",
    "`1-Tensors-in-PyTorch.ipynb:` introduces a tensor representation in PyTorch\n",
    "\n",
    "`2-Neural-networks-in-PyTorch.ipynb:` introduces a basic framework for defining neural networks via the `nn` module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks\n",
    "\n",
    "In this notebook we will explore how to train neural networks.\n",
    "\n",
    "<span style=\"color:orange\"> **IDEA:** We can think of neural networks as universal function approximators. </span>\n",
    "\n",
    "Let's consider an example below. In the middle there is some function, _F(x)_, that maps the input (images of hand-written digits) to the output (probabilities for different class labels). For instance, if we pass an image with a digit 4 to the network, we would expect to obtain probability distribution with a high likelihood corresponding to the label 4. The magic of neural networks is that we can train them with non-linear activations to approximate this function _F(x)_ successfully. \n",
    "\n",
    "<img src=\"assets/function_approx.png\" width=600px>\n",
    "\n",
    "\n",
    "<span style=\"color:blue\"> **GOAL:** We want to train the network by showing it lots of examples of digits and then adjust the weight parameters such that our network can approximate this function successfully.  </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how do we do that? To find the optimal weight parameters, we need to know how well our network is prediciting real outputs. Here we can calculate **loss function** (also called cost or optimizing function), which serves as a measure of our prediction error.\n",
    "\n",
    "There are several different types of loss functions but one of the most widely used one is the **mean squared error** (MSE). MSE is often used in regression and binary classification problems. The formula for MSE is:\n",
    "\n",
    "$$\n",
    "\\large \\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of training examples, $y_i$ are the true labels, and $\\hat{y}_i$ are the predicted labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\"> **IDEA:** We can adjust the weight parameters such that this loss is minimized. </span> Once the loss is minimized we know that out network is making as good predictions as it can.\n",
    "\n",
    "<span style=\"color:green\"> **CONCEPT:** We find the minimum loss using a process called **gradient descent**. </span> The gradient is the slope of the loss function with respect to the weight parameters. The gradient always points to the direction of the fastest change. For instance, consider the picture of the mountain below:\n",
    "\n",
    "<img src='assets/gradient_descent.png' width=350px>\n",
    "\n",
    "In this picture, the gradient is always going to point up the mountain. Imagine that our loss function is approximated by this mountain where we have the highest loss at the peak of the mountain and the lowest loss down in the valley. Therefore, if we want to minimize the loss, we have to go downwards and follow the direction of the negative gradient. You can think of this like descending a mountain by following the steepest slope to the base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
